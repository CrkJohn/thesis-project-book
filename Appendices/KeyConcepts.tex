\chapter{Conceptos importantes} % Main appendix title
\label{appendix:keyconcepts} % For referencing this appendix elsewhere, use \ref{AppendixA}

La mayor parte de este apéndice se basa en contenido expuesto en \cite{deeplearning}.

\section{Capacidad, \glsentrylong{overfitting} y \glsentrylong{underfitting}}
% \todo[inline]{Por hacer, pagina 107 Deep Learning}
El reto central de \gls{machinel} es que nuestro algoritmo tenga un buen rendimiento sobre datos nuevos, entradas nunca antes vistas, no solo las entradas con las que entrenamos nuestro modelo. La habilidad tener un buen rendimiento sobre nuevos datos se le llama \textbf{generalización}.

Típicamente cuando entrenamos un modelo de \gls{machinel} debemos acceder a un conjunto de entrenamiento; podemos computar una medida de error sobre el conjunto de entrenamiento, llamado el \textbf{error de entrenamiento}; y reducimos este error de entrenamiento. El \textbf{error de generalización} se define como el valor esperado de error sobre una nueva entrada de datos. Aquí el valor esperado se toma sobre diferentes entradas posibles, tomadas de una distribución de entradas que esperamos que el sistema se encontrara en la practica.

Usualmente el error de generalización de un modelo de \gls{machinel} se estima por su rendimiento en un \textbf{conjunto de prueba} que se obtuvieron de manera separada al conjunto de entrenamiento.

El conjunto de datos de entrenamiento y de prueba son generados por una distribucion de probabilidad sobre los conjuntos de datos llamado el \textbf{proceso de generación de datos} (o \textsl{data-generating process} en inglés).

Por lo general asumimos una serie de suposiciones (llamadas usualmente en inglés como \textsl{i.i.d. assumptions}), y es que una muestra de cada conjunto de datos es \textbf{independiente} una de otra y que los conjuntos de entrenamiento y prueba son \textbf{distribuidos idénticamente} tomados de la misma distribución de probabilidad. Esta suposición nos permite describir el proceso generativo de datos con una distribución de probabilidad sobre una sola muestra. Esta misma distribución es luego usada para generar cualquier muestra de entrenamiento y prueba. Llamamos a esta distribución compartida la \textbf{distribución generadora de datos} (o \textsl{data-generating distribution} en inglés), denotada como $\pdata$.

Cuando usamos una algoritmo de \gls{machinel}, no establecemos los parámetros de antemano para luego tomar muestras de ambos conjuntos de datos. Tomamos muestras del conjunto de entrenamiento, luego ajustamos los parámetros para reducir el error en el conjunto de entrenamiento, y luego tomamos muestras del conjunto de prueba. Bajo este proceso, el valor esperado de error en el conjunto de prueba es igual o mayor al del conjunto de entrenamiento. Los factores que determinan que tan bien un algoritmo de \gls{machinel} se desempeñara son su habilidad para
\begin{itemize}
\item Hacer que el error en el conjunto de entrenamiento sea pequeño.
\item Hacer que la diferencia entre el error de prueba y de entrenamiento sea pequeño.
\end{itemize}

Estos dos factores corresponden a los dos grandes retos en \gls{machinel}: \textbf{\gls{underfitting}} y \textbf{\gls{overfitting}}. \gls{underfitting} es cuando el modelo no es capaz de obtener un valor de error suficientemente bajo sobre el conjunto de entrenamiento. \gls{overfitting} ocurre cuando la diferencia entre el valor de error del conjunto de entrenamiento y el conjunto de prueba es muy grande.

Podemos controlar si un modelo es mas propenso a hacer \gls{underfitting} u \gls{overfitting} cambiando su \textbf{capacidad}. Informalmente, la capacidad de un modelo es su habilidad de acomodarse a una gran variedad de funciones. Modelos con baja capacidad pueden tener problemas para adaptarse al conjunto de entrenamiento. Modelos con una alta capacidad pueden adaptarse demasiado por memorizar propiedades del conjunto de entrenamiento que no le son útiles con el conjunto de pruebas.

Una forma de controlar la capacidad de nuestro algoritmo de \gls{machinel} es escogiendo su \textbf{espacio de hipótesis}, el conjunto de funciones que el algoritmo de aprendizaje se le es permitido tomar como función solución.

Un ejemplo de esto es regresión lineal, y es que podemos permitir que nuestro algoritmo de aprendizaje en vez de tomar funciones lineales (de grado $p=1$, \equationref{eq:linear-function}), también pueda tomar funciones polinomiales (grado $p > 1$, \equationref{eq:poly-function}).

\begin{equation} \label{eq:linear-function}
  \hat{y} = b + wx
\end{equation}

\begin{equation} \label{eq:poly-function}
  \hat{y} = b + \sum_{i=1}^{p} w_i x^i
\end{equation}

En este ejemplo, es útil mencionar que existe un teorema que dictamina que para cualquier entero $n \ge 0$ y una lista de $n+1$ puntos en $\R^2$ $(x_0, y_0), \ldots, (x_n, y_n)$, donde $x_0 < x_1 < \cdots < x_n$, existe un polinomio $f$ de grado $n$ tal que $f(x_i) = y_i$ para todo $i$ \cite[pág 15]{kun_2018}. Análogamente los algoritmos con una gran capacidad, existe siempre una función que se adapta perfectamente a los datos de entrenamiento, sin embargo esta función probablemente no sera útil para los datos de prueba. En la practica no siempre es posible obtener una función que se adapte perfectamente a los datos.

Los algoritmos de \gls{machinel} por lo general se desempeñan mejor cuando su capacidad es apropiada para la verdadera complejidad de la tarea que se necesita desarrollar y la cantidad apropiada de datos de entrenamiento que le son provistos.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Figures/capacity-comparison.pdf}
  \decoRule
  \caption[Comparación de capacidades]{Comparación de capacidades. Tomado de \cite{deeplearning}.}
  \label{fig:capacity-comparison}
\end{figure}

Existen muchas maneras de cambiar la capacidad de un modelo, la capacidad no esta solo determinada por la elección del modelo. El modelo especifica cual familia de funciones se pueden escoger cuando se varían los parámetros para alcanzar un objetivo de entrenamiento. A esto se le llama la \textbf{capacidad representativa} (o \textsl{representational capacity} en inglés) del modelo. En muchos casos encontrar la mejor función dentro de esta familia es un problema difícil de optimización. En la practica, el algoritmo de aprendizaje no encuentra la mejor función, sino una que meramente reduce el error de entrenamiento. Estas limitaciones adicionales, tales como la imperfección del algoritmo de optimización, significa que la \textbf{capacidad efectiva} del algoritmo puede ser menor al de la capacidad representativa.

\subsection{Teorema ``No free lunch''}
La teoría de aprendizaje dice que un algoritmo de \gls{machinel} puede generalizar de manera aceptable a partir de un numero finito de muestras de conjuntos de entrenamiento. Esto parece contradecir algunos principios básicos de la lógica. Inferir reglas generales de un conjunto de muestras limitadas no es lógicamente valido. Para inferir lógicamente una regla para describir cada miembro de un conjunto es necesario tener información de todos los miembros de ese conjunto.

\gls{machinel} evita es problema por medio de ofrecer reglas probabilísticas, en vez de ofrecer reglas completamente ciertas que ofrece el razonamiento lógico. \gls{machinel} promete encontrar reglas que son \textit{probablemente} ciertas para la \textit{mayor} parte de los miembros de un conjunto que les concierne.

Desafortunadamente, esto no resuelve el problema completo. Existe un teorema en \gls{machinel} llamado ``No free lunch'' que afirma que, promediando sobre todas las posibles distribuciones generadoras de datos, cualquier algoritmo de clasificación tiene la misma tasa de error cuando clasifica puntos antes no vistos. Esto quiere decir que no existe un algoritmo de \gls{machinel} que es mejor universalmente a los demás.

Sin embargo, esto solo es aplicable cuando se habla del promedio de \textit{todas} las posibles distribuciones generadoras de datos. Si hacemos suposiciones sobre los tipos de distribución de probabilidad que nos encontramos en la vida real, entonces podemos diseñar algoritmos de aprendizaje que se desempeñan mejor sobre esas distribuciones.

Esto significa que el objetivo de la investigación en \gls{machinel} no busca encontrar un algoritmo de aprendizaje universal, sino que el objetivo es encontrar que tipos de distribuciones son relevantes al ``mundo real'' que un agente de \gls{ai} experimenta, y que tipos de algoritmos de \gls{machinel} se desempeñan bien con datos tomados del tipo de distribuciones generadoras de datos que nos interesan.

\subsection{Regularización}
El teorema de ``no free lunch'' nos dice que debemos diseñar nuestro algoritmo de \gls{machinel} para tener un buen rendimiento una tarea especifica, esto lo logramos por medio de tener una serie de preferencias dentro del algoritmo de aprendizaje, cuando estas preferencias se alinean hacia el objetivo que queremos lograr este tiene un mejor desempeño.

Aunque una forma de lograr ese objetivo se puede lograr en parte por medio de modificar la capacidad, no es la única forma de lograrlo. El comportamiento de nuestro algoritmo esta fuertemente influenciado no solo por que tan grande hacemos el conjunto de funciones permitido en su espacio hipótesis, sino también la identidad especifica de esas funciones.

Una método popular para realizar un ajuste de forma que se controle el \gls{underfitting} y el \gls{overfitting} es por medio de modificar nuestra función de costo $J$ agregándole un hiper-parámetro $\lambda$ que se establece de antemano sobre el algoritmo de manera \textit{heurística}. Modificamos $J$ de manera que cuando $\lambda = 0$ no se modifique el valor original de $J$, permitiéndonos controlar el nivel de \gls{underfitting} y \gls{overfitting} cambiando el valor de $\lambda$.

De manera mas general, podemos regularizar un modelo que aprende una función $f(\vx, \vtheta)$ por medio de añadir una penalidad llamada \textbf{regulador} (o \textsl{regularizer} en inglés) a la función de costo $J$. Como ejemplo, téngase una función hipótesis $\hat{f}(\vx, \vtheta) = \vtheta^{\top} \vx$ con parámetros $\vtheta$ y una función objetivo $f(\vx)$ de manera que podemos definir una función de costo que queremos minimizar como:
\begin{equation}
  J(\vx, \vtheta) = | \hat{f}(\vx, \vtheta) - f(\vx) |
\end{equation}

Agregando $\lambda$ a la función de costo, la ecuación queda como:
\begin{equation}
  J(\vx, \vtheta) = | \hat{f}(\vx, \vtheta) - f(\vx) | + \lambda \vtheta^{\top} \vtheta
\end{equation}

De esta manera penalizamos los valores grandes de $\vtheta$ (cuando $\lambda > 0$) que pueden ocasionar \gls{overfitting} de forma que regularizan el avance del algoritmo de optimización que se usa para minimizar la función de costo. Sin embargo, un valor muy grande en $\lambda$ ocasionara \gls{underfitting}.

% ================================================================
% ================================================================

\section{Hiper-parámetros y conjuntos de Validación}
\todo[inline]{Por hacer, pagina 117 Deep Learning}

\subsection{Validación cruzada}
\todo[inline]{Por hacer}

% ================================================================
% ================================================================

\section{Estimadores, Parcialidad (\glsentrylong{bias}) y Varianza}
% \todo[inline]{Por hacer, pagina 119 Deep Learning}
El campo de las estadísticas nos da muchas herramientas para conseguir nuestro objetivo de \gls{machinel} de resolver una tarea no solo con el conjunto de entrenamiento sino que también para generalizar. Conceptos fundamentales como la estimación de puntos, la parcialidad y la varianza son útiles para caracterizar formalmente las nociones de generalidad, \gls{underfitting} y \gls{overfitting}.

\subsection{Estimación de puntos}
La estimación de puntos es el intento de proveer la única ``mejor'' predicción de alguna cantidad de interés. En general esta cantidad de interés puede ser un único parámetro o un vector de parámetros en algún modelo paramétrico.

Para distinguir estimados de parámetros de su valor real se utiliza la distinción de un parámetro $\vtheta$ con $\hat{\vtheta}$.

Sea $\{ \vx^{(1)}, \ldots, \vx^{(m)} \}$ un conjunto de $m$ puntos de datos independiente e idénticamente distribuidos. Un \textbf{estimador} o \textbf{estadístico} es cualquier función en base a los datos:
\begin{equation}
  \hat{\vtheta}_m = g( \vx^{(1)}, \ldots, \vx^{(m)} )
\end{equation}

Sin embargo, la definición no requiere que $g$ sea una buena estimación del verdadero $\vtheta$ ni tampoco que la respuesta este en el mismo conjunto de valores validos de $\vtheta$. Esta definición supone una gran flexibilidad para definir un estimador, sin embargo un buen estimador es una función cuya salida sea cercana al verdadero $\vtheta$ que genero los datos de entrenamiento.

Podemos asumir (desde la perspectiva de frecuentista) que el valor verdadero del parámetro $\vtheta$ es fijo pero desconocido, y que $\hat{\vtheta}$ es una estimación en función a los datos.

\subsubsection{Estimación de funciones}
La estimación de puntos también puede hacer referencia a la estimación de la relación entre variables entrada y objetivo, nos referimos a estos tipos de estimación de puntos como estimación de funciones.

En este escenario tratamos de predecir una variable $\vy$ dado un vector de entrada $\vx$. Asumimos que existe una función $f(\vx)$ que describe una relación aproximada entre $\vy$ con $\vx$. Podemos asumir que $\vy = f(\vx) + \vepsilon$, donde $\vepsilon$ es la parte de $\vy$ que no es predecible a partir de $\vx$. En la estimación de funciones nos interesa aproximar $f$ con un modelo o estimado $\hat{f}$.

\subsection{Parcialidad (\glsentrylong{bias})}
La parcialidad (o \gls{bias} en inglés) es un estimador definido como:
\begin{equation}
  \mathrm{bias}(\hat{\vtheta}_m) = \E(\hat{\vtheta}_m) - \vtheta
\end{equation}

Donde la el valor esperado es sobre los datos (vistos como muestras de una variable aleatoria), donde $\vtheta$ es el valor verdadero de $\vtheta$ usado para definir la distribución que generadora de los datos. Un estimador se dice que es imparcial (o \textsl{unbiased} en inglés) si $\mathrm{bias}(\hat{\vtheta}_m) = \vzero$, lo cual implica que $\E(\hat{\vtheta}_m) = \vtheta$. Un estimador se dice que es \textbf{asintótico imparcial} si $\lim_{m\to\infty}\mathrm{bias}(\hat{\vtheta}_m) = \vzero$.

Los estimadores imparciales son ciertamente deseables, sin embargo no son siempre los ``mejores'' estimadores.

\subsection{Varianza y Error Estándar}
Otra propiedad de un estimador que puede que debamos tener en cuenta es que tanto se espera que van a variar nuestros resultados como función de la muestra de datos.

La \textbf{varianza} de un estimador se define como:
\begin{equation}
  \Var(\hat{\rvtheta})
\end{equation}

donde la variable aleatoria $\hat{\rvtheta}$ es el conjunto de entrenamiento. De manera alternativa la raíz cuadrada de la varianza es el \textbf{error estándar}, denotado como $\standarderror(\hat{\rvtheta})$.

La varianza (o el error estándar) de un estimador nos provee de una medida de como esperaríamos que el estimado que computamos de los datos variara cuando retomamos una muestra independiente del conjunto de datos. De la misma manera en que nosotros querríamos una parcialidad baja, querríamos también una varianza baja.

El error estándar de la media esta dado por:
\begin{equation}
  \standarderror(\hat{\mu}_m) = \sqrt{ \Var{\Bigg[ \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \Bigg]} } = \frac{\sigma}{\sqrt{m}}
\end{equation}
    
La varianza de un estimador disminuye en función de $m$, el numero de muestras de nuestro conjunto de datos.

\subsection{Consistencia}
En la labor de escoger un mejor estimador nos interesa especialmente como este se comportara a medida que la cantidad de datos de entrenamiento aumenta. En particular nos interesa que a medida que la cantidad de datos aumenta nuestros estimados se aproximan cada vez mas al verdadero valor de los parámetros. Formalmente deseamos:

\begin{equation} \label{eq:consistency}
  \plim_{m\to\infty} \hat{\rvtheta} = \rvtheta
\end{equation}

El símbolo $\plim$ indica convergencia en probabilidad, indicando que para cualquier $\epsilon > 0$ se tiene que $P(|\hat{\rvtheta} - \rvtheta| > \epsilon) \to 0$ cuando $m \to \infty$. La condición descrita en la \equationref{eq:consistency} se le conoce como \textbf{consistencia}.

La consistencia nos asegura que la parcialidad inducida por el estimador se elimina a medida que la cantidad de muestras de datos aumenta. Sin embargo lo contrario no es verdad --- imparcialidad asintótica no implica consistencia.

A medida que la capacidad del modelo aumenta, la parcialidad tiende a disminuir y la varianza tiende a aumentar, dándonos así una curva en forma de U para el error de generalización, tal como se muestra en la \figureref{fig:consistency}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/consistency.pdf}
  \decoRule
  \caption[Consistencia]{Consistencia. Tomado de \cite{deeplearning}.}
  \label{fig:consistency}
\end{figure}

% ================================================================
% ================================================================

\section{Curva \glsentryname{roc} y valor \glsentryname{auc}}
\todo[inline]{Por hacer, usar \cite{Zou2007}, esto se necesitara para los resultados de los experimentos}
\gls{roc} y \gls{auc}
